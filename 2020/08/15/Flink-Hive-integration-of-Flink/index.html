<!DOCTYPE html>
<html lang="en">
    <!-- title -->




<!-- keywords -->




<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" >
    <meta name="author" content="Jie Wang">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Jie Wang">
    
    <meta name="keywords" content="hexo,hexo-theme,hexo-blog">
    
    <meta name="description" content="Jack's personel blog">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    <title>Flink Hive Integration · Jack Wang&#39;s Blog</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href= "/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    <link rel="stylesheet" href= "/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href= "/assets/favicon.ico" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script" />
    <link rel="preload" href="/scripts/main.js" as="script" />
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
<meta name="generator" content="Hexo 5.4.2"></head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Jack Wang&#39;s Blog</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Flink Hive Integration</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Jack Wang's Blog</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Flink Hive Integration
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "Flink">Flink</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "Connector">Connector</a>
    
</div>
                
                
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count word-count">3.3k</span>Reading time: <span class="post-count reading-time">17 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2020/08/15</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <p>从Flink 1.10开始，支持对Hive的集成，相关的代码位于flink-connectors&#x2F;flink-connector-hive中。下面从读取和写入Hive数据两个方面对Hive集成进行分析。</p>
<h2 id="读取Hive数据"><a href="#读取Hive数据" class="headerlink" title="读取Hive数据"></a>读取Hive数据</h2><h3 id="InputSplit和InputFormat"><a href="#InputSplit和InputFormat" class="headerlink" title="InputSplit和InputFormat"></a>InputSplit和InputFormat</h3><p>InputSplit是Flink任务处理数据的基本单位：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">InputSplit</span> <span class="keyword">extends</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line">	<span class="comment">// Returns the number of this input split.</span></span><br><span class="line">	<span class="type">int</span> <span class="title function_">getSplitNumber</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>而InputFormat是Flink的DataSource的基础接口，用于根据并行度要求，将输入数据划分和创建多个Splits。InputFormat继承自InputSplitSource接口，用于处理：</p>
<ul>
<li>描述输入数据怎么切换成可以并行处理的InputSplit</li>
<li>描述如何从Split中读取Records</li>
<li>描述输入数据的基本统计信息</li>
</ul>
<p>InputFormat的生命周期如下：</p>
<ol>
<li>在实例化后，通过<code>configure(Configuration parameters)</code>进行配置，parameters包含了关于数据基本的信息，例如文件的路径等</li>
<li>（可选）编译器调用<code>getStatistics()</code>获取输入数据的统计信息</li>
<li>通过<code>createInputSplits(int minNumSplits)</code>创建Splits</li>
<li>每个并行的输入任务都会创建和配置一个InputFormat实例，并用之打开一个特定的Split进行读取和处理</li>
<li>关闭InputFormat实例</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">InputFormat</span>&lt;OT, T <span class="keyword">extends</span> <span class="title class_">InputSplit</span>&gt; <span class="keyword">extends</span> <span class="title class_">InputSplitSource</span>&lt;T&gt;, Serializable &#123;</span><br><span class="line">	<span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Configuration parameters)</span>;</span><br><span class="line">	BaseStatistics <span class="title function_">getStatistics</span><span class="params">(BaseStatistics cachedStatistics)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line">	</span><br><span class="line">  <span class="comment">// InputSplitSource</span></span><br><span class="line">	<span class="comment">// Creates the different splits of the input that can be processed in parallel.</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	T[] createInputSplits(<span class="type">int</span> minNumSplits) <span class="keyword">throws</span> IOException;</span><br><span class="line">	<span class="comment">// Gets the type of the input splits that are processed by this input format.</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	InputSplitAssigner <span class="title function_">getInputSplitAssigner</span><span class="params">(T[] inputSplits)</span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// For a specific split</span></span><br><span class="line">	<span class="keyword">void</span> <span class="title function_">open</span><span class="params">(T split)</span> <span class="keyword">throws</span> IOException</span><br><span class="line">	<span class="type">boolean</span> <span class="title function_">reachedEnd</span><span class="params">()</span> <span class="keyword">throws</span> IOException;</span><br><span class="line">	OT <span class="title function_">nextRecord</span><span class="params">(OT reuse)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line">	<span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：不同的InputFormat，对应的不同的InputSplit类型以及记录类型，便于扩展。</p>
<h4 id="那么Split与Task是如何进行分配的呢？"><a href="#那么Split与Task是如何进行分配的呢？" class="headerlink" title="那么Split与Task是如何进行分配的呢？"></a>那么Split与Task是如何进行分配的呢？</h4><p>InputFormat的<code>getInputSplitAssigner()</code>方法会返回InputSplitAssigner接口的实例。InputSplitAssigner用于完成对任务进行分配InputSplit：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">InputSplitAssigner</span> &#123;</span><br><span class="line">	<span class="comment">// Returns the next input split that shall be consumed. The consumer&#x27;s host is passed as a parameter to allow localized assignments.</span></span><br><span class="line">	InputSplit <span class="title function_">getNextInputSplit</span><span class="params">(String host, <span class="type">int</span> taskId)</span>;</span><br><span class="line">  </span><br><span class="line">	<span class="comment">// Return the splits to assigner if the task failed to process it.</span></span><br><span class="line">	<span class="keyword">void</span> <span class="title function_">returnInputSplit</span><span class="params">(List&lt;InputSplit&gt; splits, <span class="type">int</span> taskId)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>InputSplitAssigner有多种实现，主要有：</p>
<ul>
<li>DefaultInputSplitAssigner：依次多所有的InputSplits进行分配，并不考虑位置信息</li>
<li>LocatableInputSplitAssigner：优先分配Locality较高的Split给任务进行处理（最常用）</li>
<li>ReplicatingInputSplitAssigner：每一个InputSplit都将被所有的任务处理</li>
</ul>
<h3 id="HiveTableInputFormat"><a href="#HiveTableInputFormat" class="headerlink" title="HiveTableInputFormat"></a>HiveTableInputFormat</h3><p>Hive数据的读取是通过HiveTableInputFormat来完成，HiveTableInputFormat是HadoopInputFormatCommanBase的子类。</p>
<p>![image-20191218222449636](&#x2F;Users&#x2F;jie.wang&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20191218222449636.png)</p>
<p>HadoopInputFormatCommanBase是所有基于Hadoop的公共父类，保存了访问Hadoop的credential信息。而RichInputFormat包含了用于RuntimeContext，从而可以访问运行时的上下文信息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HiveTableInputFormat</span> <span class="keyword">extends</span> <span class="title class_">HadoopInputFormatCommonBase</span>&lt;BaseRow, HiveTableInputSplit&gt; </span><br></pre></td></tr></table></figure>

<p>HiveTableInputFormat的记录使用BaseRow类型表示，InputSplit类型则为HiveTableInputSplit。BaseRow是一个接口，根据不同行的类型有：ColumnarRow，UpdatableRow，BinaryRow，JoinedRow，NestedRow，ObjectArrayRow等实现。</p>
<p>HiveTableInputFormat在构造时，会从CatalogTable获取关于Partition等信息：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">HiveTableInputFormat</span><span class="params">(</span></span><br><span class="line"><span class="params">  JobConf jobConf,</span></span><br><span class="line"><span class="params">  CatalogTable catalogTable,</span></span><br><span class="line"><span class="params">  List&lt;HiveTablePartition&gt; partitions,</span></span><br><span class="line"><span class="params">  <span class="type">int</span>[] projectedFields,</span></span><br><span class="line"><span class="params">  <span class="type">long</span> limit,</span></span><br><span class="line"><span class="params">  String hiveVersion)</span> &#123;</span><br><span class="line">  <span class="built_in">super</span>(jobConf.getCredentials()); <span class="comment">// For hadoop credentials</span></span><br><span class="line">  <span class="built_in">this</span>.partitionKeys = catalogTable.getPartitionKeys();</span><br><span class="line">  <span class="built_in">this</span>.fieldTypes = catalogTable.getSchema().getFieldDataTypes();</span><br><span class="line">  <span class="built_in">this</span>.fieldNames = catalogTable.getSchema().getFieldNames();</span><br><span class="line">  <span class="built_in">this</span>.limit = limit;</span><br><span class="line">  <span class="built_in">this</span>.hiveVersion = hiveVersion;</span><br><span class="line">  checkNotNull(catalogTable, <span class="string">&quot;catalogTable can not be null.&quot;</span>);</span><br><span class="line">  <span class="built_in">this</span>.partitions = checkNotNull(partitions, <span class="string">&quot;partitions can not be null.&quot;</span>);</span><br><span class="line">  <span class="built_in">this</span>.jobConf = <span class="keyword">new</span> <span class="title class_">JobConf</span>(jobConf);</span><br><span class="line">  <span class="type">int</span> <span class="variable">rowArity</span> <span class="operator">=</span> catalogTable.getSchema().getFieldCount();</span><br><span class="line">  selectedFields = projectedFields != <span class="literal">null</span> ? projectedFields : IntStream.range(<span class="number">0</span>, rowArity).toArray();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="createInputSplits-amp-getInputSplitAssigner"><a href="#createInputSplits-amp-getInputSplitAssigner" class="headerlink" title="createInputSplits() &amp; getInputSplitAssigner()"></a>createInputSplits() &amp; getInputSplitAssigner()</h4><p>HiveTableInputFormat在创建HiveTableInputSplit时，会根据输入的HiveTablePartition获取对应的InputFormation，从而获取每个Partition包含的mapred.InputSplit，并创建HiveTableInputSplit：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> HiveTableInputSplit[] createInputSplits(<span class="type">int</span> minNumSplits)</span><br><span class="line">  <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  List&lt;HiveTableInputSplit&gt; hiveSplits = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">  <span class="type">int</span> <span class="variable">splitNum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">  <span class="comment">// Iterate over the partitions to get splits of each partition.</span></span><br><span class="line">  <span class="keyword">for</span> (HiveTablePartition partition : partitions) &#123;</span><br><span class="line">    <span class="type">StorageDescriptor</span> <span class="variable">sd</span> <span class="operator">=</span> partition.getStorageDescriptor();</span><br><span class="line">    InputFormat format;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      format = (InputFormat)</span><br><span class="line">        Class.forName(sd.getInputFormat(), <span class="literal">true</span>, Thread.currentThread().getContextClassLoader()).newInstance();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">FlinkHiveException</span>(<span class="string">&quot;Unable to instantiate the hadoop input format&quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    ReflectionUtils.setConf(format, jobConf);</span><br><span class="line">    <span class="comment">// Set mapreduce.input.fileinputformat.inputdir </span></span><br><span class="line">    jobConf.set(INPUT_DIR, sd.getLocation()); </span><br><span class="line"> </span><br><span class="line">    org.apache.hadoop.mapred.InputSplit[] splitArray = format.getSplits(jobConf, minNumSplits);</span><br><span class="line">    <span class="keyword">for</span> (org.apache.hadoop.mapred.InputSplit inputSplit : splitArray) &#123;</span><br><span class="line">      <span class="comment">// Why we make a copy of jonConf</span></span><br><span class="line">      hiveSplits.add(<span class="keyword">new</span> <span class="title class_">HiveTableInputSplit</span>(splitNum++, inputSplit, jobConf, partition));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> hiveSplits.toArray(<span class="keyword">new</span> <span class="title class_">HiveTableInputSplit</span>[<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>InputSplitAssigner则是利用LocatableInputSplitAssigner：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">InputSplitAssigner</span> getInputSplitAssigner(<span class="type">HiveTableInputSplit</span>[] inputSplits) &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">new</span> <span class="type">LocatableInputSplitAssigner</span>(inputSplits);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="HiveTableInputSplit"><a href="#HiveTableInputSplit" class="headerlink" title="HiveTableInputSplit"></a>HiveTableInputSplit</h4><p>HiveTableInputSplit的继承路径为：InputSplit-&gt;LocatableInputSplit-&gt;HadoopInputSplit-&gt;HiveTableInputSplit。其中，LocatableInputSplit记录了保存对应Split的节点（hosts）信息，HadoopInputSplit持有一个org.apache.hadoop.mapred.InputSplit实例和JobConf，从而可以从Hadoop上读取对应的Split数据。</p>
<p>HiveTableInputSplit则在HadoopInputSplit上记录了HiveTablePartition信息：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HiveTableInputSplit</span> <span class="keyword">extends</span> <span class="title class_">HadoopInputSplit</span> &#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> HiveTablePartition hiveTablePartition;</span><br><span class="line">	<span class="keyword">public</span> <span class="title function_">HiveTableInputSplit</span><span class="params">(</span></span><br><span class="line"><span class="params">			<span class="type">int</span> splitNumber,</span></span><br><span class="line"><span class="params">			InputSplit hInputSplit,</span></span><br><span class="line"><span class="params">			JobConf jobconf,</span></span><br><span class="line"><span class="params">			HiveTablePartition hiveTablePartition)</span> &#123;</span><br><span class="line">		<span class="built_in">super</span>(splitNumber, hInputSplit, jobconf);</span><br><span class="line">		<span class="built_in">this</span>.hiveTablePartition = checkNotNull(hiveTablePartition, <span class="string">&quot;hiveTablePartition can not be null&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HiveTablePartition描述了Hive表的一个分区，如果Hive表不是一个分区表，那么HiveTablePartition就表示整个表。HiveTablePartition包含：</p>
<ul>
<li><p>StorageDescriptor：Thrift对象，可以与Hive进行通信，包含了存储相关的所有信息，包括Column Types，SerDe，Compress，Format等：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> List&lt;FieldSchema&gt; cols; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> String location; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> String inputFormat; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> String outputFormat; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> <span class="type">boolean</span> compressed; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> <span class="type">int</span> numBuckets; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> SerDeInfo serdeInfo; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> List&lt;String&gt; bucketCols; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> List&lt;Order&gt; sortCols; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> Map&lt;String,String&gt; parameters; <span class="comment">// required</span></span><br><span class="line"> <span class="keyword">private</span> SkewedInfo skewedInfo; <span class="comment">// optional</span></span><br><span class="line"> <span class="keyword">private</span> <span class="type">boolean</span> storedAsSubDirectories; <span class="comment">// optional</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Map&lt;String, Object&gt; partitionSpec：partition键以及该Partition中的值。</p>
</li>
</ul>
<h4 id="读取InputSplit"><a href="#读取InputSplit" class="headerlink" title="读取InputSplit"></a>读取InputSplit</h4><p>Flink任务在利用HiveTableInputFormat读取某个InputSplit时，首先通过<code>open()</code>方法创建SplitReader。在没有设置<code>table.exec.hive.fallback-mapred-reader</code>并且利用<code>useOrcVectorizedRead()</code>判断selectedFields为Flink兼容的类型时，会创建HiveVectorizedOrcSplitReader，否则创建HiveMapredSplitReader。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!parameters.getBoolean(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER) &amp;&amp;</span><br><span class="line">useOrcVectorizedRead(split.getHiveTablePartition())) &#123;</span><br><span class="line">  <span class="built_in">this</span>.reader = <span class="keyword">new</span> <span class="title class_">HiveVectorizedOrcSplitReader</span>(</span><br><span class="line">  jobConf, fieldNames, fieldTypes, selectedFields, split);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">this</span>.reader = <span class="keyword">new</span> <span class="title class_">HiveMapredSplitReader</span>(jobConf, partitionKeys, fieldTypes, selectedFields, split,</span><br><span class="line">  HiveShimLoader.loadHiveShim(hiveVersion));</span><br><span class="line">&#125;</span><br><span class="line">currentReadCount = <span class="number">0L</span>;</span><br></pre></td></tr></table></figure>

<p>HiveVectorizedOrcSplitReader只支持的基本类型，不支持ARRAY，MAP，MULTISET，INTERVAL等类型。而HiveMapredSplitReader因为使用的是Hive提供的MapRedSplitReader，所以支持所有Hive类型。</p>
<p>记录的读取都是通过SplitReader完成，针对提供limit的情况，会提前判断是否结束：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> BaseRow <span class="title function_">nextRecord</span><span class="params">(BaseRow reuse)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  currentReadCount++;</span><br><span class="line">  <span class="keyword">return</span> reader.nextRecord(reuse);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">reachedEnd</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  <span class="keyword">if</span> (limit &gt; <span class="number">0</span> &amp;&amp; currentReadCount &gt;= limit) &#123; <span class="comment">// limit is set, and hit the limit, early-stop</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> reader.reachedEnd();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>HiveMapredSplitReader</strong></p>
<p>HiveMapredSplitReader在初始化时，会根据StorageDescriptor获取对应的InputFormat，通过InputFormat获取对应的mapred.RecordReader：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">HiveMapredSplitReader</span><span class="params">(...)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  <span class="built_in">this</span>.hiveTablePartition = split.getHiveTablePartition();</span><br><span class="line">  <span class="type">StorageDescriptor</span> <span class="variable">sd</span> <span class="operator">=</span> hiveTablePartition.getStorageDescriptor();</span><br><span class="line">  jobConf.set(INPUT_DIR, sd.getLocation());</span><br><span class="line">  InputFormat mapredInputFormat;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    mapredInputFormat = (InputFormat)</span><br><span class="line">      Class.forName(sd.getInputFormat(), <span class="literal">true</span>, Thread.currentThread().getContextClassLoader()).newInstance();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;&#125;</span><br><span class="line">  ReflectionUtils.setConf(mapredInputFormat, jobConf);</span><br><span class="line">  ....</span><br><span class="line">  <span class="comment">// Get RecordReader from InputFormat</span></span><br><span class="line">  <span class="built_in">this</span>.recordReader = mapredInputFormat.getRecordReader(split.getHadoopInputSplit(),</span><br><span class="line">                                                          jobConf, <span class="keyword">new</span> <span class="title class_">HadoopDummyReporter</span>());</span><br><span class="line">  ..</span><br><span class="line">  <span class="comment">// Initialize the deserializer for record:</span></span><br><span class="line">  key = <span class="built_in">this</span>.recordReader.createKey();</span><br><span class="line">  value = <span class="built_in">this</span>.recordReader.createValue();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    deserializer = (Deserializer) Class.forName(sd.getSerdeInfo().getSerializationLib()).newInstance();</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="comment">//properties are used to initialize hive Deserializer properly.</span></span><br><span class="line">    <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> HiveTableUtil.createPropertiesFromStorageDescriptor(sd);</span><br><span class="line">    SerDeUtils.initializeSerDe(deserializer, conf, properties, <span class="literal">null</span>);</span><br><span class="line">    <span class="comment">// Use ObjectInspector to get StructFields</span></span><br><span class="line">    structObjectInspector = (StructObjectInspector) deserializer.getObjectInspector();</span><br><span class="line">    structFields = structObjectInspector.getAllStructFieldRefs();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">FlinkHiveException</span>(<span class="string">&quot;Error happens when deserialize from storage file.&quot;</span>, e);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">this</span>.selectedFields = selectedFields;</span><br><span class="line">  <span class="built_in">this</span>.partitionKeys = partitionKeys;</span><br><span class="line">  <span class="comment">// Used to convert each field to Flink Object</span></span><br><span class="line">  converters = Arrays.stream(selectedFields)</span><br><span class="line">    .mapToObj(i -&gt; fieldTypes[i])</span><br><span class="line">    .map(DataFormatConverters::getConverterForDataType)</span><br><span class="line">    .toArray(DataFormatConverters.DataFormatConverter[]::<span class="keyword">new</span>);</span><br><span class="line">  <span class="built_in">this</span>.hiveShim = hiveShim;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在<code>nextRecord()</code>读取时，首先通过RecordReader获取下一条记录，对值利用deserializer反序列化后，并依次对选择等待列利用Inspector和converters转换成对应的Flink对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> BaseRow <span class="title function_">nextRecord</span><span class="params">(BaseRow reuse)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  <span class="keyword">if</span> (reachedEnd()) &#123; <span class="comment">// Will try to fetch next record: hasNext = recordReader.next(key, value);</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">final</span> <span class="type">GenericRow</span> <span class="variable">row</span> <span class="operator">=</span> reuse <span class="keyword">instanceof</span> GenericRow ?</span><br><span class="line">    (GenericRow) reuse : <span class="keyword">new</span> <span class="title class_">GenericRow</span>(selectedFields.length);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">//Use HiveDeserializer to deserialize an object out of a Writable blob</span></span><br><span class="line">    <span class="type">Object</span> <span class="variable">hiveRowStruct</span> <span class="operator">=</span> deserializer.deserialize(value);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; selectedFields.length; i++) &#123;</span><br><span class="line">      <span class="comment">// set non-partition columns</span></span><br><span class="line">      <span class="keyword">if</span> (selectedFields[i] &lt; structFields.size()) &#123;</span><br><span class="line">        <span class="type">StructField</span> <span class="variable">structField</span> <span class="operator">=</span> structFields.get(selectedFields[i]);</span><br><span class="line">        <span class="comment">// To FlinkObject</span></span><br><span class="line">        <span class="type">Object</span> <span class="variable">object</span> <span class="operator">=</span> HiveInspectors.toFlinkObject(structField.getFieldObjectInspector(),</span><br><span class="line">                                                     structObjectInspector.getStructFieldData(hiveRowStruct, structField), hiveShim);</span><br><span class="line">        <span class="comment">// Convert it to target internal type</span></span><br><span class="line">        row.setField(i, converters[i].toInternal(object));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;...&#125;</span><br><span class="line">  <span class="comment">// If reuse row, only set the partition keys for the first time:</span></span><br><span class="line">  <span class="keyword">if</span> (!rowReused) &#123;</span><br><span class="line">    <span class="comment">// set partition columns</span></span><br><span class="line">    <span class="keyword">if</span> (!partitionKeys.isEmpty()) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; selectedFields.length; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (selectedFields[i] &gt;= structFields.size()) &#123;</span><br><span class="line">          <span class="type">String</span> <span class="variable">partition</span> <span class="operator">=</span> partitionKeys.get(selectedFields[i] - structFields.size());</span><br><span class="line">          row.setField(i, converters[i].toInternal(hiveTablePartition.getPartitionSpec().get(partition)));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    rowReused = <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">this</span>.fetched = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">return</span> row;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>HiveVectorizedOrcSplitReader</strong></p>
<p>HiveVectorizedOrcSplitReader内部则直接使用的是OrcColumnarRowSplitReader来读取：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">this</span>.reader = genPartColumnarRowReader(</span><br><span class="line">				conf,</span><br><span class="line">				fieldNames,</span><br><span class="line">				fieldTypes,</span><br><span class="line">				split.getHiveTablePartition().getPartitionSpec(),</span><br><span class="line">				selectedFields,</span><br><span class="line">				<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(),</span><br><span class="line">				DEFAULT_SIZE,</span><br><span class="line">				<span class="keyword">new</span> <span class="title class_">Path</span>(fileSplit.getPath().toString()),</span><br><span class="line">				fileSplit.getStart(),</span><br><span class="line">				fileSplit.getLength());</span><br></pre></td></tr></table></figure>

<h3 id="HiveTableSource"><a href="#HiveTableSource" class="headerlink" title="HiveTableSource"></a>HiveTableSource</h3><p>Flink并不能直接使用HiveInputFormat进行处理，而是通过HiveTableSource将读取的数据封装为DataStream，以便供其下游使用。HiveTableSource实现了PartitionTableSource, ProjectableTableSource和LimitableTableSource接口，尽量将过滤条件下推，减少读取不必要的分区。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HiveTableSource</span> <span class="keyword">implements</span></span><br><span class="line">  <span class="title class_">StreamTableSource</span>&lt;BaseRow&gt;, <span class="comment">// HiveTableSource is a bounded source(batch)</span></span><br><span class="line">PartitionableTableSource, <span class="comment">// Partition pruning </span></span><br><span class="line">ProjectableTableSource&lt;BaseRow&gt;, <span class="comment">// Only return projected fields</span></span><br><span class="line">LimitableTableSource&lt;BaseRow&gt; &#123; <span class="comment">// For limit push down</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> JobConf jobConf;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ObjectPath tablePath; <span class="comment">// Database and table name</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CatalogTable catalogTable;</span><br><span class="line">  <span class="comment">// Remaining partition specs after partition pruning is performed. Null if pruning is not pushed down.</span></span><br><span class="line">  <span class="meta">@Nullable</span></span><br><span class="line">  <span class="keyword">private</span> List&lt;Map&lt;String, String&gt;&gt; remainingPartitions = <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">private</span> String hiveVersion;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">boolean</span> partitionPruned;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">int</span>[] projectedFields;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">isLimitPushDown</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">long</span> <span class="variable">limit</span> <span class="operator">=</span> -<span class="number">1L</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HiveTableSource核心的方法为<code>getDataStream(execEnv)</code>，用于返回，个<code>DataStream&lt;BaseRaw&gt;</code>。<code>getDataStream(execEnv)</code>：</p>
<ol>
<li>首先会连接到Hivemetastore，获取需要的分区</li>
<li>创建底层的HiveInputFormat，以及DataStreamSource</li>
<li>根据Split的数量为DataStreamSource设置并行度，并构造DataStreamSource的名字</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> DataStream&lt;BaseRow&gt; <span class="title function_">getDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> &#123;</span><br><span class="line">  <span class="comment">// Get needed partitions based on remainingPartitions, if remainin gPartitions are not set, return all partitions from Hivemetasotre</span></span><br><span class="line">  List&lt;HiveTablePartition&gt; allHivePartitions = initAllPartitions();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Apply the projection and construct the projected typeinfo.</span></span><br><span class="line">  <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">  TypeInformation&lt;BaseRow&gt; typeInfo =</span><br><span class="line">    (TypeInformation&lt;BaseRow&gt;) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(getProducedDataType());</span><br><span class="line">  <span class="type">HiveTableInputFormat</span> <span class="variable">inputFormat</span> <span class="operator">=</span> getInputFormat(allHivePartitions);</span><br><span class="line">  <span class="comment">// Create the DataStreamSource </span></span><br><span class="line">  DataStreamSource&lt;BaseRow&gt; source = execEnv.createInput(inputFormat, typeInfo);</span><br><span class="line"></span><br><span class="line">  <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> GlobalConfiguration.loadConfiguration();</span><br><span class="line">  <span class="keyword">if</span> (conf.getBoolean(HiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM)) &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">max</span> <span class="operator">=</span> conf.getInteger(HiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM_MAX);</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// Infer the parallelism based on num of splits</span></span><br><span class="line">    splitNum = inputFormat.createInputSplits(<span class="number">0</span>).length;</span><br><span class="line">    source.setParallelism(Math.min(Math.max(<span class="number">1</span>, splitNum), max));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> source.name(explainSource());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="从Hive-Metastore获取partitions"><a href="#从Hive-Metastore获取partitions" class="headerlink" title="从Hive Metastore获取partitions"></a>从Hive Metastore获取partitions</h4><p><code>allHivePartitions()</code>会连接到Hivemetastore，根据Hive Table是否分区进行处理，如果是分区表，只返回remainingPartitions中的分区。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> List&lt;HiveTablePartition&gt; <span class="title function_">initAllPartitions</span><span class="params">()</span> &#123;</span><br><span class="line">  List&lt;HiveTablePartition&gt; allHivePartitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">  <span class="keyword">try</span> (<span class="type">HiveMetastoreClientWrapper</span> <span class="variable">client</span> <span class="operator">=</span> HiveMetastoreClientFactory.create(<span class="keyword">new</span> <span class="title class_">HiveConf</span>(jobConf, HiveConf.class), hiveVersion)) &#123;</span><br><span class="line">    ...</span><br><span class="line">    List&lt;String&gt; partitionColNames = catalogTable.getPartitionKeys();</span><br><span class="line">    <span class="comment">// Partitioned table</span></span><br><span class="line">    <span class="keyword">if</span> (partitionColNames != <span class="literal">null</span> &amp;&amp; partitionColNames.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      ...</span><br><span class="line">      List&lt;Partition&gt; partitions = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">      <span class="keyword">if</span> (remainingPartitions != <span class="literal">null</span>) &#123; <span class="comment">// Only get the pruned partitions</span></span><br><span class="line">        <span class="keyword">for</span> (Map&lt;String, String&gt; spec : remainingPartitions) &#123;</span><br><span class="line">          partitions.add(client.getPartition(dbName, tableName, partitionSpecToValues(spec, partitionColNames)));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        partitions.addAll(client.listPartitions(dbName, tableName, (<span class="type">short</span>) -<span class="number">1</span>));</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// Build the partitionColValues for HiveTablePartition</span></span><br><span class="line">      <span class="keyword">for</span> (Partition partition : partitions) &#123;</span><br><span class="line">        <span class="type">StorageDescriptor</span> <span class="variable">sd</span> <span class="operator">=</span> partition.getSd();</span><br><span class="line">        Map&lt;String, Object&gt; partitionColValues = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; partitionColNames.size(); i++) &#123;</span><br><span class="line">          <span class="type">String</span> <span class="variable">partitionColName</span> <span class="operator">=</span> partitionColNames.get(i);</span><br><span class="line">          <span class="type">String</span> <span class="variable">partitionValue</span> <span class="operator">=</span> partition.getValues().get(i);</span><br><span class="line">          <span class="type">DataType</span> <span class="variable">type</span> <span class="operator">=</span> catalogTable.getSchema().getFieldDataType(partitionColName).get();</span><br><span class="line">          Object partitionObject;</span><br><span class="line">          <span class="comment">// The values of dynamic partitions or the values contains non-scape character, the partition values are set to defaultPartitionName `__HIVE_DEFAULT_PARTITION__`</span></span><br><span class="line">          <span class="keyword">if</span> (defaultPartitionName.equals(partitionValue)) &#123;</span><br><span class="line">            <span class="type">LogicalTypeRoot</span> <span class="variable">typeRoot</span> <span class="operator">=</span> type.getLogicalType().getTypeRoot();</span><br><span class="line">            <span class="comment">// while this is inline with Hive, seems it should be null for string columns as well</span></span><br><span class="line">            partitionObject = typeRoot == LogicalTypeRoot.CHAR || typeRoot == LogicalTypeRoot.VARCHAR ? defaultPartitionName : <span class="literal">null</span>;</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partitionObject = restorePartitionValueFromFromType(partitionValue, type);</span><br><span class="line">          &#125;</span><br><span class="line">          partitionColValues.put(partitionColName, partitionObject);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">HiveTablePartition</span> <span class="variable">hiveTablePartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HiveTablePartition</span>(sd, partitionColValues);</span><br><span class="line">        allHivePartitions.add(hiveTablePartition);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// For non partitioned table</span></span><br><span class="line">      allHivePartitions.add(<span class="keyword">new</span> <span class="title class_">HiveTablePartition</span>(client.getTable(dbName, tableName).getSd()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> allHivePartitions;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="创建DataStream"><a href="#创建DataStream" class="headerlink" title="创建DataStream"></a>创建DataStream</h4><p>DataStreamSource通过StreamExecutionEnvironment的<code>createInput()</code>创建，针对FileInputFormat，会通过<code>createFileInput()</code>创建：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;OUT&gt; DataStreamSource&lt;OUT&gt; <span class="title function_">createInput</span><span class="params">(InputFormat&lt;OUT, ?&gt; inputFormat, TypeInformation&lt;OUT&gt; typeInfo)</span> &#123;</span><br><span class="line">  DataStreamSource&lt;OUT&gt; source;</span><br><span class="line">  <span class="keyword">if</span> (inputFormat <span class="keyword">instanceof</span> FileInputFormat) &#123; <span class="comment">// HiveTableSource is a FileInputFormat</span></span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    FileInputFormat&lt;OUT&gt; format = (FileInputFormat&lt;OUT&gt;) inputFormat;</span><br><span class="line"></span><br><span class="line">    source = createFileInput(format, typeInfo, <span class="string">&quot;Custom File source&quot;</span>,</span><br><span class="line">                             FileProcessingMode.PROCESS_ONCE, -<span class="number">1</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    source = createInput(inputFormat, typeInfo, <span class="string">&quot;Custom Source&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> source;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>createFileInput()</code>会创建一个ContinuousFileMonitoringFunction用于监控输入的路径，并创建FileInputSplits给下游的ContinuousFileReaderOperator，ContinuousFileReaderOperator会读取实际的数据。ContinuousFileMonitoringFunction在创建所有FileInputSplits后就退出，并不会等待所有的FileInputSplits读取完成。因此，对于文件输入，只能有一个Checkpoint。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;OUT&gt; DataStreamSource&lt;OUT&gt; <span class="title function_">createFileInput</span><span class="params">(FileInputFormat&lt;OUT&gt; inputFormat,</span></span><br><span class="line"><span class="params">                                                    TypeInformation&lt;OUT&gt; typeInfo,</span></span><br><span class="line"><span class="params">                                                    String sourceName,</span></span><br><span class="line"><span class="params">                                                    FileProcessingMode monitoringMode,</span></span><br><span class="line"><span class="params">                                                    <span class="type">long</span> interval)</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  Preconditions.checkArgument(monitoringMode.equals(FileProcessingMode.PROCESS_ONCE) ||</span><br><span class="line">                              interval &gt;= ContinuousFileMonitoringFunction.MIN_MONITORING_INTERVAL,</span><br><span class="line">                              <span class="string">&quot;The path monitoring interval cannot be less than &quot;</span> +</span><br><span class="line">                              ContinuousFileMonitoringFunction.MIN_MONITORING_INTERVAL + <span class="string">&quot; ms.&quot;</span>);</span><br><span class="line">  ContinuousFileMonitoringFunction&lt;OUT&gt; monitoringFunction =</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ContinuousFileMonitoringFunction</span>&lt;&gt;(inputFormat, monitoringMode, getParallelism(), interval);</span><br><span class="line"></span><br><span class="line">  ContinuousFileReaderOperator&lt;OUT&gt; reader =</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ContinuousFileReaderOperator</span>&lt;&gt;(inputFormat);</span><br><span class="line"></span><br><span class="line">  SingleOutputStreamOperator&lt;OUT&gt; source = addSource(monitoringFunction, sourceName)</span><br><span class="line">    .transform(<span class="string">&quot;Split Reader: &quot;</span> + sourceName, typeInfo, reader);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">DataStreamSource</span>&lt;&gt;(source);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="写Hive数据"><a href="#写Hive数据" class="headerlink" title="写Hive数据"></a>写Hive数据</h2><h3 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h3><p>OutputFormat用于消费数据，描述了如何存储记录数据，是所有输出的公共接口。生命周期如下：</p>
<ol>
<li>通过<code>configure()</code>配置OutputFormat，用于初始化</li>
<li>每一个并行的输出任务都会创建对应的OutputFormat实例，配置并打开(<code>open()</code>)</li>
<li>通过<code>writeRecord()</code>输出记录数据</li>
<li>利用<code>close()</code>关闭OutputFormat</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">OutputFormat</span>&lt;IT&gt; <span class="keyword">extends</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Configuration parameters)</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Opens a parallel instance of the output format to store the result of its parallel instance</span></span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(<span class="type">int</span> taskNumber, <span class="type">int</span> numTasks)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">writeRecord</span><span class="params">(IT record)</span> <span class="keyword">throws</span> IOException;</span><br><span class="line">  <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>常用的OutputFormat有：</p>
<ul>
<li>FileSystemOutputFormat</li>
<li>BlockingShuffleOutputFormat</li>
<li>LoggerOutputFormat&#x2F;DiscardingOutputFormat</li>
<li>HiveOutputFormat</li>
</ul>
<h3 id="HiveOutputFormatFactory-x2F-HiveOutputFormat"><a href="#HiveOutputFormatFactory-x2F-HiveOutputFormat" class="headerlink" title="HiveOutputFormatFactory&#x2F;HiveOutputFormat"></a>HiveOutputFormatFactory&#x2F;HiveOutputFormat</h3><p>HiveOutputFormatFactory实现了OutputFormatFactory接口，通过<code>createOutputFormat(path)</code>创建一个对应的HiveOutputFormat。在创建HiveOutputFormat时需要：</p>
<ol>
<li>初始化HiveOutputFormatFactory<ol>
<li>根据serDeInfo加载并创建SerDer实例，并利用<code>SerDeUtils.initializeSerDe()</code>对其初始化</li>
<li>获取非分区列的HiveObjectConversions和ObjectInspectors，并创建最终的StructObjectInspector。HiveObjectConversion用于将Flink对象类型转换成对应的Hive对象类型。StructObjectInspector则用于SerDe序列化。</li>
</ol>
</li>
<li>为MapRed.FileOutputFormat在JobConf添加和配置压缩信息：<ol>
<li>hive.intermediate.compression.codec：设置CompressionCodec的Class</li>
<li>hive.intermediate.compression.type：设置SequenceFile.CompressionType的Class</li>
</ol>
</li>
<li>创建RecordWriter和HiveOutputFormat</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> HiveOutputFormat <span class="title function_">createOutputFormat</span><span class="params">(Path outPath)</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1. load SerDer and create HiveObjectConversions and StructObjectInspector</span></span><br><span class="line">      <span class="keyword">if</span> (!inited) &#123;</span><br><span class="line">        init();</span><br><span class="line">        inited = <span class="literal">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 2. configure the compress</span></span><br><span class="line">      <span class="type">JobConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JobConf</span>(confWrapper.conf());</span><br><span class="line">      <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">isCompressed</span> <span class="operator">=</span> conf.getBoolean(HiveConf.ConfVars.COMPRESSRESULT.varname, <span class="literal">false</span>);</span><br><span class="line">      <span class="keyword">if</span> (isCompressed) &#123;</span><br><span class="line">        ...</span><br><span class="line">        FileOutputFormat.setOutputCompressorClass(conf, codec);</span><br><span class="line">        SequenceFileOutputFormat.setOutputCompressionType(conf, style);</span><br><span class="line">        ....</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">		<span class="comment">// 3. Create the recordWriter</span></span><br><span class="line">    <span class="type">RecordWriter</span> <span class="variable">recordWriter</span> <span class="operator">=</span> hiveShim.getHiveRecordWriter(</span><br><span class="line">      conf,</span><br><span class="line">      outputFormat, <span class="comment">//The MapRed OutputFormat, eg: Parquet, ORC..</span></span><br><span class="line">      recordSerDe.getSerializedClass(),</span><br><span class="line">      isCompressed,</span><br><span class="line">      tableProperties,</span><br><span class="line">      HadoopFileSystem.toHadoopPath(outPath));</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">HiveOutputFormat</span>(recordWriter);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HiveOutputFormat的实现就非常简单，利用RecordWriter写入记录即可。</p>
<h3 id="HiveTableSink"><a href="#HiveTableSink" class="headerlink" title="HiveTableSink"></a>HiveTableSink</h3><p>HiveTableSink继承自<code>OutputFormatTableSink</code>，支持分区和Overwrite：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HiveTableSink</span> <span class="keyword">extends</span> <span class="title class_">OutputFormatTableSink</span>&lt;Row&gt; <span class="keyword">implements</span> <span class="title class_">PartitionableTableSink</span>, OverwritableTableSink &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> JobConf jobConf;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CatalogTable catalogTable;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ObjectPath tablePath;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> TableSchema tableSchema;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String hiveVersion;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> HiveShim hiveShim;</span><br><span class="line">  <span class="comment">// Partition infos</span></span><br><span class="line">  <span class="keyword">private</span> LinkedHashMap&lt;String, String&gt; staticPartitionSpec = <span class="keyword">new</span> <span class="title class_">LinkedHashMap</span>&lt;&gt;();</span><br><span class="line">  <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">overwrite</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">dynamicGrouping</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HiveTableSink利用<code>getOutputFormat()</code>创建OutputFormat：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> OutputFormat&lt;Row&gt; <span class="title function_">getOutputFormat</span><span class="params">()</span> &#123;</span><br><span class="line">		String[] partitionColumns = getPartitionFieldNames().toArray(<span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">0</span>]);</span><br><span class="line">		<span class="type">String</span> <span class="variable">dbName</span> <span class="operator">=</span> tablePath.getDatabaseName();</span><br><span class="line">		<span class="type">String</span> <span class="variable">tableName</span> <span class="operator">=</span> tablePath.getObjectName();</span><br><span class="line">		<span class="keyword">try</span> (<span class="type">HiveMetastoreClientWrapper</span> <span class="variable">client</span> <span class="operator">=</span> HiveMetastoreClientFactory.create(</span><br><span class="line">				<span class="keyword">new</span> <span class="title class_">HiveConf</span>(jobConf, HiveConf.class), hiveVersion)) &#123;</span><br><span class="line">			<span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> client.getTable(dbName, tableName);</span><br><span class="line">			<span class="type">StorageDescriptor</span> <span class="variable">sd</span> <span class="operator">=</span> table.getSd();</span><br><span class="line"></span><br><span class="line">			FileSystemOutputFormat.Builder&lt;Row&gt; builder = <span class="keyword">new</span> <span class="title class_">FileSystemOutputFormat</span>.Builder&lt;&gt;();</span><br><span class="line">			builder.setPartitionComputer(<span class="keyword">new</span> <span class="title class_">RowPartitionComputer</span>(</span><br><span class="line">					jobConf.get(</span><br><span class="line">							HiveConf.ConfVars.DEFAULTPARTITIONNAME.varname,</span><br><span class="line">							HiveConf.ConfVars.DEFAULTPARTITIONNAME.defaultStrVal),</span><br><span class="line">					tableSchema.getFieldNames(),</span><br><span class="line">					partitionColumns));</span><br><span class="line">			builder.setDynamicGrouped(dynamicGrouping);</span><br><span class="line">			builder.setPartitionColumns(partitionColumns);</span><br><span class="line">			builder.setFileSystemFactory(<span class="keyword">new</span> <span class="title class_">HadoopFileSystemFactory</span>(jobConf)); </span><br><span class="line">			builder.setFormatFactory(<span class="keyword">new</span> <span class="title class_">HiveOutputFormatFactory</span>( <span class="comment">// Create the HiveOutputFormat</span></span><br><span class="line">					jobConf,</span><br><span class="line">					sd.getOutputFormat(),</span><br><span class="line">					sd.getSerdeInfo(),</span><br><span class="line">					tableSchema,</span><br><span class="line">					partitionColumns,</span><br><span class="line">					HiveReflectionUtils.getTableMetadata(hiveShim, table),</span><br><span class="line">					hiveShim));</span><br><span class="line">			builder.setMetaStoreFactory(</span><br><span class="line">					<span class="keyword">new</span> <span class="title class_">HiveTableMetaStoreFactory</span>(jobConf, hiveVersion, dbName, tableName));</span><br><span class="line">			builder.setOverwrite(overwrite);</span><br><span class="line">			builder.setStaticPartitions(staticPartitionSpec);</span><br><span class="line">			builder.setTempPath(<span class="keyword">new</span> <span class="title class_">org</span>.apache.flink.core.fs.Path(</span><br><span class="line">					toStagingDir(sd.getLocation(), jobConf)));</span><br><span class="line"></span><br><span class="line">			<span class="keyword">return</span> builder.build();</span><br><span class="line">		&#125; ...</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>


    </article>
    <!-- license  -->
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href= "/2020/08/23/Spark-Spark-Shuffle/" title= "Spark Shuffle">
                    <div class="nextTitle">Spark Shuffle</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2020/07/23/Spark-Spark-Broadcast/" title= "Spark Broadcast">
                    <div class="prevTitle">Spark Broadcast</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    <div id="disqus_thread"></div>
    <script>
        /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        
        var disqus_config = function () {
        this.page.url = "http://jackwangcs.github.io/2020/08/15/Flink-Hive-integration-of-Flink/";  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = "Flink Hive Integration"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        
        (function () { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://https-jackwangcs-github-io.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();

    </script>
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

    
    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:jackwangcs@outlook.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/jackwangcs" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- mermaid support  -->
    
    <script src='https://unpkg.com/mermaid@8.4.2/dist/mermaid.min.js'></script>
    <script>
        mermaid.initialize({ theme: 'dark' });
    </script>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96Hive%E6%95%B0%E6%8D%AE"><span class="toc-number">1.</span> <span class="toc-text">读取Hive数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#InputSplit%E5%92%8CInputFormat"><span class="toc-number">1.1.</span> <span class="toc-text">InputSplit和InputFormat</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%82%A3%E4%B9%88Split%E4%B8%8ETask%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%88%86%E9%85%8D%E7%9A%84%E5%91%A2%EF%BC%9F"><span class="toc-number">1.1.1.</span> <span class="toc-text">那么Split与Task是如何进行分配的呢？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HiveTableInputFormat"><span class="toc-number">1.2.</span> <span class="toc-text">HiveTableInputFormat</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#createInputSplits-amp-getInputSplitAssigner"><span class="toc-number">1.2.1.</span> <span class="toc-text">createInputSplits() &amp; getInputSplitAssigner()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HiveTableInputSplit"><span class="toc-number">1.2.2.</span> <span class="toc-text">HiveTableInputSplit</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96InputSplit"><span class="toc-number">1.2.3.</span> <span class="toc-text">读取InputSplit</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HiveTableSource"><span class="toc-number">1.3.</span> <span class="toc-text">HiveTableSource</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8EHive-Metastore%E8%8E%B7%E5%8F%96partitions"><span class="toc-number">1.3.1.</span> <span class="toc-text">从Hive Metastore获取partitions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BADataStream"><span class="toc-number">1.3.2.</span> <span class="toc-text">创建DataStream</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%99Hive%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">写Hive数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#OutputFormat"><span class="toc-number">2.1.</span> <span class="toc-text">OutputFormat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HiveOutputFormatFactory-x2F-HiveOutputFormat"><span class="toc-number">2.2.</span> <span class="toc-text">HiveOutputFormatFactory&#x2F;HiveOutputFormat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HiveTableSink"><span class="toc-number">2.3.</span> <span class="toc-text">HiveTableSink</span></a></li></ol></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 51
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2023 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/16</span><a class="archive-post-title" href= "/2023/01/16/Flink-Flink-Unified-Sink/" >Flink Unified Sink</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2022 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/16</span><a class="archive-post-title" href= "/2022/11/16/Flink-Flink-Deduplicate-Functions/" >Flink DeduplicateFunction</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/19</span><a class="archive-post-title" href= "/2022/06/19/Flink-Flink-MiniBatch/" >Flink MiniBatch</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/15</span><a class="archive-post-title" href= "/2022/02/15/Yarn-YarnSchedulerAdvances/" >Yarn 3.0 Scheduling Advances</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/13</span><a class="archive-post-title" href= "/2022/02/13/Yarn-Yarn3-0-Features/" >Yarn 3.0+ New Features</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/15</span><a class="archive-post-title" href= "/2022/01/15/Hudi-Flink-Write/" >Flink Integration with Hudi</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2021 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/23</span><a class="archive-post-title" href= "/2021/12/23/Hudi/" >Hudi Overview</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/03</span><a class="archive-post-title" href= "/2021/12/03/Calcite-Calcite-VolcanoPlanner/" >Calcite Volcano Planner</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/01</span><a class="archive-post-title" href= "/2021/12/01/Calcite-Calcite-Planner/" >Calcite Planner</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/23</span><a class="archive-post-title" href= "/2021/11/23/Calcite-Calcite/" >Calcite Overview</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/09</span><a class="archive-post-title" href= "/2021/08/09/Flink-Failure-Recovery/" >Flink Restart and Recovery</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/16</span><a class="archive-post-title" href= "/2021/07/16/Flink-Flink-TaskManager/" >Flink TaskManager</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/03</span><a class="archive-post-title" href= "/2021/05/03/Flink-Flink-ClassLoader/" >Flink ClassLoader</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/23</span><a class="archive-post-title" href= "/2021/02/23/Flink-Flink-Planner/" >Flink Planner</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/03</span><a class="archive-post-title" href= "/2021/02/03/Flink-Flink-SQL/" >Flink SQL</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/07</span><a class="archive-post-title" href= "/2021/01/07/Flink-Flink-TableSource-and-TableSink/" >Flink TableSource and TableSink</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2020 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/10</span><a class="archive-post-title" href= "/2020/11/10/Spark-spark-sql/" >Spark SQL Basics</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/03</span><a class="archive-post-title" href= "/2020/11/03/Flink-Flink-Scheduler/" >Flink Scheduler</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/02</span><a class="archive-post-title" href= "/2020/11/02/Flink-flink-file-system-connector/" >Flink FileSystem Connector</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/17</span><a class="archive-post-title" href= "/2020/09/17/Calcite-Planner/" >Calcite SQL Planner</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/17</span><a class="archive-post-title" href= "/2020/09/17/Flink-Flink-DataStream/" >Flink DataStream</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/17</span><a class="archive-post-title" href= "/2020/09/17/Flink-FlinkUnifiedMemory/" >Flink Unified Memory</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/10</span><a class="archive-post-title" href= "/2020/09/10/Spark-Spark-Context-and-Env/" >Spark Context and Env</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span><a class="archive-post-title" href= "/2020/08/23/Flink-Flink-ExecutionGraph/" >Flink ExecutionGraph</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span><a class="archive-post-title" href= "/2020/08/23/Flink-Flink-JobManager/" >Flink JobManager</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/23</span><a class="archive-post-title" href= "/2020/08/23/Spark-Spark-Shuffle/" >Spark Shuffle</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/15</span><a class="archive-post-title" href= "/2020/08/15/Flink-Hive-integration-of-Flink/" >Flink Hive Integration</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span><a class="archive-post-title" href= "/2020/07/23/Spark-Spark-Broadcast/" >Spark Broadcast</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/15</span><a class="archive-post-title" href= "/2020/07/15/Spark-Spark-TaskScheduler-and-Backend/" >Spark TaskScheduler and Backend</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/10</span><a class="archive-post-title" href= "/2020/05/10/Spark-SparkQueryExecution/" >Spark Query Execution</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/15</span><a class="archive-post-title" href= "/2020/04/15/Spark-Spark-RDD/" >Spark RDD</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/11</span><a class="archive-post-title" href= "/2020/02/11/Spark-Spark-UnifiedMemoryManager/" >Spark Unified Memory Manager</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/20</span><a class="archive-post-title" href= "/2019/10/20/Spark-spark-internal/" >Spark Internals Basics</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/18</span><a class="archive-post-title" href= "/2019/10/18/Spark-Spark-BlockManager/" >Spark BlockManager</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span><a class="archive-post-title" href= "/2019/09/22/Spark-Spark-Tungsten/" >Spark Tungsten</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/10</span><a class="archive-post-title" href= "/2019/09/10/Streaming-Window/" >Streaming Windows</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/28</span><a class="archive-post-title" href= "/2019/07/28/Yarn-CapacityScheduler/" >CapacityScheduler</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/16</span><a class="archive-post-title" href= "/2019/07/16/Yarn-ResourceScheduler/" >Yarn Resource Scheduler</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/16</span><a class="archive-post-title" href= "/2019/07/16/Yarn-%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8/" >Yarn ResourceScheduler</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/13</span><a class="archive-post-title" href= "/2019/07/13/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/" >Linux常用命令总结</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/01</span><a class="archive-post-title" href= "/2019/07/01/Yarn-%E5%AE%89%E5%85%A8%E7%AE%A1%E7%90%86/" >Hadoop安全管理</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/28</span><a class="archive-post-title" href= "/2019/06/28/Yarn-ResourceManager%E5%AE%9E%E7%8E%B0/" >ResourceManager实现</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/28</span><a class="archive-post-title" href= "/2019/06/28/Yarn-ResourceManager%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/" >ResourceManager行为分析</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/19</span><a class="archive-post-title" href= "/2019/06/19/Streaming-What-where-when-and-how-of-data-processing/" >What, where, when and how of data processing</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/11</span><a class="archive-post-title" href= "/2019/06/11/Streaming-watermarks/" >Watermarks</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/25</span><a class="archive-post-title" href= "/2019/05/25/Streaming-Exactly-Once-and-Side-Effects/" >Streaming Exactly Once</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/24</span><a class="archive-post-title" href= "/2019/05/24/Yarn-%E5%9F%BA%E7%A1%80%E5%BA%93/" >Yarn 基础库</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/21</span><a class="archive-post-title" href= "/2019/05/21/HDFS-NameNode%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E5%81%9C%E6%AD%A2/" >NameNode的启动和停止</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/21</span><a class="archive-post-title" href= "/2019/05/21/Yarn-%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E5%92%8C%E6%9E%B6%E6%9E%84/" >Yarn架构</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/18</span><a class="archive-post-title" href= "/2019/05/18/Streaming-Streaming-101/" >Streaming 101</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/08</span><a class="archive-post-title" href= "/2019/05/08/streaming-101/" >Streaming 101</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="Streaming System"><span class="iconfont-archer">&#xe606;</span>Streaming System</span>
    
        <span class="sidebar-tag-name" data-tags="Calcite"><span class="iconfont-archer">&#xe606;</span>Calcite</span>
    
        <span class="sidebar-tag-name" data-tags="SQL"><span class="iconfont-archer">&#xe606;</span>SQL</span>
    
        <span class="sidebar-tag-name" data-tags="Hudi"><span class="iconfont-archer">&#xe606;</span>Hudi</span>
    
        <span class="sidebar-tag-name" data-tags="DataLake"><span class="iconfont-archer">&#xe606;</span>DataLake</span>
    
        <span class="sidebar-tag-name" data-tags="Flink"><span class="iconfont-archer">&#xe606;</span>Flink</span>
    
        <span class="sidebar-tag-name" data-tags="Calclite"><span class="iconfont-archer">&#xe606;</span>Calclite</span>
    
        <span class="sidebar-tag-name" data-tags="Linux"><span class="iconfont-archer">&#xe606;</span>Linux</span>
    
        <span class="sidebar-tag-name" data-tags="Commands"><span class="iconfont-archer">&#xe606;</span>Commands</span>
    
        <span class="sidebar-tag-name" data-tags="Runtime"><span class="iconfont-archer">&#xe606;</span>Runtime</span>
    
        <span class="sidebar-tag-name" data-tags="Core"><span class="iconfont-archer">&#xe606;</span>Core</span>
    
        <span class="sidebar-tag-name" data-tags="Connector"><span class="iconfont-archer">&#xe606;</span>Connector</span>
    
        <span class="sidebar-tag-name" data-tags="Table"><span class="iconfont-archer">&#xe606;</span>Table</span>
    
        <span class="sidebar-tag-name" data-tags="Planner"><span class="iconfont-archer">&#xe606;</span>Planner</span>
    
        <span class="sidebar-tag-name" data-tags="Memory"><span class="iconfont-archer">&#xe606;</span>Memory</span>
    
        <span class="sidebar-tag-name" data-tags="HDFS"><span class="iconfont-archer">&#xe606;</span>HDFS</span>
    
        <span class="sidebar-tag-name" data-tags="NameNode"><span class="iconfont-archer">&#xe606;</span>NameNode</span>
    
        <span class="sidebar-tag-name" data-tags="Hadoop"><span class="iconfont-archer">&#xe606;</span>Hadoop</span>
    
        <span class="sidebar-tag-name" data-tags="Streaming"><span class="iconfont-archer">&#xe606;</span>Streaming</span>
    
        <span class="sidebar-tag-name" data-tags="Exactly-Once"><span class="iconfont-archer">&#xe606;</span>Exactly-Once</span>
    
        <span class="sidebar-tag-name" data-tags="Watermarks"><span class="iconfont-archer">&#xe606;</span>Watermarks</span>
    
        <span class="sidebar-tag-name" data-tags="Spark"><span class="iconfont-archer">&#xe606;</span>Spark</span>
    
        <span class="sidebar-tag-name" data-tags="Scheduler"><span class="iconfont-archer">&#xe606;</span>Scheduler</span>
    
        <span class="sidebar-tag-name" data-tags="DataFrame"><span class="iconfont-archer">&#xe606;</span>DataFrame</span>
    
        <span class="sidebar-tag-name" data-tags="Yarn"><span class="iconfont-archer">&#xe606;</span>Yarn</span>
    
        <span class="sidebar-tag-name" data-tags="ResourceScheduler"><span class="iconfont-archer">&#xe606;</span>ResourceScheduler</span>
    
        <span class="sidebar-tag-name" data-tags="ResourceManager"><span class="iconfont-archer">&#xe606;</span>ResourceManager</span>
    
        <span class="sidebar-tag-name" data-tags="NodeManager"><span class="iconfont-archer">&#xe606;</span>NodeManager</span>
    
        <span class="sidebar-tag-name" data-tags="ResourceSheduler"><span class="iconfont-archer">&#xe606;</span>ResourceSheduler</span>
    
        <span class="sidebar-tag-name" data-tags="Scheduling"><span class="iconfont-archer">&#xe606;</span>Scheduling</span>
    
        <span class="sidebar-tag-name" data-tags="RPC"><span class="iconfont-archer">&#xe606;</span>RPC</span>
    
        <span class="sidebar-tag-name" data-tags="StateMachine"><span class="iconfont-archer">&#xe606;</span>StateMachine</span>
    
        <span class="sidebar-tag-name" data-tags="Security"><span class="iconfont-archer">&#xe606;</span>Security</span>
    
        <span class="sidebar-tag-name" data-tags="Architecture"><span class="iconfont-archer">&#xe606;</span>Architecture</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="Streaming-Systemfalse"><span class="iconfont-archer">&#xe60a;</span>Streaming-Systemfalse</span>
    
        <span class="sidebar-category-name" data-categories="Calcite"><span class="iconfont-archer">&#xe60a;</span>Calcite</span>
    
        <span class="sidebar-category-name" data-categories="DataLake"><span class="iconfont-archer">&#xe60a;</span>DataLake</span>
    
        <span class="sidebar-category-name" data-categories="SQL"><span class="iconfont-archer">&#xe60a;</span>SQL</span>
    
        <span class="sidebar-category-name" data-categories="Linux"><span class="iconfont-archer">&#xe60a;</span>Linux</span>
    
        <span class="sidebar-category-name" data-categories="Flink"><span class="iconfont-archer">&#xe60a;</span>Flink</span>
    
        <span class="sidebar-category-name" data-categories="HDFS"><span class="iconfont-archer">&#xe60a;</span>HDFS</span>
    
        <span class="sidebar-category-name" data-categories="Streaming-System"><span class="iconfont-archer">&#xe60a;</span>Streaming-System</span>
    
        <span class="sidebar-category-name" data-categories="Spark"><span class="iconfont-archer">&#xe60a;</span>Spark</span>
    
        <span class="sidebar-category-name" data-categories="Yarn"><span class="iconfont-archer">&#xe60a;</span>Yarn</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "Jie Wang"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    </body>
</html>


